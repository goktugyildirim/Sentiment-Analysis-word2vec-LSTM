{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "filename= \"tweet.csv\"\n",
    "all_data = pd.read_csv(filename, encoding = \"ISO-8859-1\", header = None)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data X Shape:  140000\n",
      "All Data Y Shape:  140000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = all_data.loc[:, [5]].values\n",
    "y = all_data.loc[:, [0]].values\n",
    "\n",
    "x = [str(data) for data in list(x)]\n",
    "y = [int(data) for data in list(y)]\n",
    "\n",
    "number_of_data_in_each_class = 70000\n",
    "\n",
    "x_neg = x[:number_of_data_in_each_class]\n",
    "y_neg = y[:number_of_data_in_each_class]\n",
    "x_pos = x[850000:850000+number_of_data_in_each_class]\n",
    "y_pos = y[850000:850000+number_of_data_in_each_class]\n",
    "x = x_neg + x_pos\n",
    "y = y_neg + y_pos\n",
    "\n",
    "\n",
    "print(\"All Data X Shape: \",len(x))\n",
    "print(\"All Data Y Shape: \",len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: , <class 'list'> 140000\n",
      "<class 'str'>\n",
      "<class 'list'> 140000\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Text: ,\",type(x),len(x))\n",
    "print(type(x[0]))\n",
    "print(type(y),len(y))\n",
    "print(type(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization for Puncutuations with NLTK module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lexical analysis, tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. It helps to reveal semantic information of words. Also, it makes text cleaning easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization:  ['@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!']\n",
      "After tokenization:  ['@ iamjazzyfizzle I wish I got to watch it with you !! I miss you and @ iamlilnicki how was the premiere ?!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "def listToString(s):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(s)) \n",
    "\n",
    "print(\"Before tokenization: \",x[15])\n",
    "x = [listToString(WordPunctTokenizer().tokenize(sentence)) for sentence in x]\n",
    "print(\"After tokenization: \",x[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization for Words with NLTK module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import nltk\\n#nltk.download(\\'punkt\\')\\n\\ndef listToString(s):  \\n    str1 = \" \" \\n    return (str1.join(s)) \\n\\nprint(\"Before tokenization: \",x[15])\\nx = [listToString(nltk.word_tokenize(sentence)) for sentence in x]\\nprint(\"After tokenization: \",x[15])'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def listToString(s):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(s)) \n",
    "\n",
    "print(\"Before tokenization: \",x[15])\n",
    "x = [listToString(nltk.word_tokenize(sentence)) for sentence in x]\n",
    "print(\"After tokenization: \",x[15])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning using regular expressions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: \n",
      " ['@ iamjazzyfizzle I wish I got to watch it with you !! I miss you and @ iamlilnicki how was the premiere ?!']\n",
      "After cleaning: \n",
      "  iamjazzyfizzle i wish i got to watch it with you  i miss you and  iamlilnicki how was the premiere \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation, digits\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #Preprocess the text in a single tweet\n",
    "    #arguments: tweet = a single tweet in form of string \n",
    "    #convert the tweet to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #convert all urls to sting \"URL\"\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #convert all @username to \"AT_USER\"\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER', tweet)\n",
    "    #correct all multiple white spaces to a single white space\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #convert \"#topic\" to just \"topic\"\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    \n",
    "    converter = str.maketrans('', '', punctuation)\n",
    "    tweet = tweet.translate(converter)\n",
    "    \n",
    "    converter = str.maketrans('', '', digits)\n",
    "    tweet = tweet.translate(converter)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "print(\"Before cleaning: \\n\",x[15])\n",
    "x = [preprocess_tweet(sentence) for sentence in x]\n",
    "print(\"After cleaning: \\n\",x[15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: , <class 'list'>\n",
      "<class 'str'>\n",
      "Output:  <class 'list'> 140000\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: ,\",type(x))\n",
    "print(type(x[0]))\n",
    "print(\"Output: \",type(y),len(y))\n",
    "print(type(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Multi-label Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the polarity of the tweet (0 = negative,  4 = positive)\n",
    "num_neg = []\n",
    "num_pos = []\n",
    "for i in range(len(y)):\n",
    " \n",
    "    if y[i]==0: #negative\n",
    "        y[i] = [1,0]\n",
    "        num_neg.append((i,\"negative\"))\n",
    "    elif y[i]==4: #positive\n",
    "        y[i] = [0,1]\n",
    "        num_pos.append((i,\"positive\"))\n",
    "    else:\n",
    "        print(\"Hata\")\n",
    "\n",
    "x = [sentence.split() for sentence in x] # list in list structure because of Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: , <class 'list'> 140000\n",
      "<class 'list'>\n",
      "Output:  <class 'list'> 140000\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: ,\",type(x),len(x))\n",
    "print(type(x[0]))\n",
    "print(\"Output: \",type(y),len(y))\n",
    "print(type(y[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "min_count = 5\n",
    "window = 2\n",
    "output_size = 120\n",
    "feature_epoch = 70\n",
    "model_name = \"1\"\n",
    "\n",
    "#train-test split\n",
    "split_ratio = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab. length:  14017\n",
      "Time to build vocab: 0.01 mins\n",
      "Removing is done. 0.99 mins\n",
      "Time to train the model: 1.34 mins\n",
      "Maximum token length in each sample:  60\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def Word2vecFeatureExtraction(input, y, min_count,window, output_size, epoch, model_name):\n",
    "    from gensim.models import Word2Vec\n",
    "    import multiprocessing\n",
    "    from time import time  # To time our operations\n",
    "    \n",
    "    #create model\n",
    "    model = Word2Vec(min_count = min_count,window=window,size=output_size,sample=6e-5,alpha=0.03,min_alpha=0.0007,negative=20,workers=4,compute_loss=True)\n",
    "    t = time()\n",
    "    model.build_vocab(input, progress_per=10000)\n",
    "    vocab = list(model.wv.vocab)\n",
    "    #print(\"Vocabularies:\\n**********************************************\\n\",vocab)\n",
    "    print(\"Vocab. length: \",len(vocab))\n",
    "    print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    \n",
    "\n",
    "    # remove out-of-vocabulary words\n",
    "    t = time()\n",
    "    all_data = []\n",
    "    y_last = []\n",
    "    \n",
    "    \n",
    "    for i, sentence in enumerate(input):\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                new_sentence.append(word)\n",
    "        if len(new_sentence)!=0:\n",
    "            all_data.append(new_sentence)\n",
    "            y_last.append(y[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    print(\"Removing is done. {} mins\".format(round((time() - t) / 60, 2)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #train the Word2vec model\n",
    "    t = time()\n",
    "    model.train(all_data, total_examples = model.corpus_count, epochs = epoch, report_delay=1)\n",
    "    model.save(\"epoch_\" + str(epoch) + \"_output_size_\"+str(output_size)+\"_\"+ model_name + \".bin\")\n",
    "    print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    \n",
    "    \n",
    "    tokens_lengths = [len(model.wv[value]) for value in all_data]\n",
    "    print(\"Maximum token length in each sample: \", max(tokens_lengths))\n",
    "    \n",
    "    features = [model.wv[value] for value in all_data]\n",
    "    #model = Word2Vec.load(model_name)\n",
    "    \n",
    "    return features,y_last,max(tokens_lengths), model\n",
    "\n",
    "\n",
    "x_features,y,max_tokens_lengths, model = Word2vecFeatureExtraction(x, y, min_count, window, output_size, feature_epoch, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139569 139569\n"
     ]
    }
   ],
   "source": [
    "print(len(x_features), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Word2vec, length of sentence is: 19 \n",
      " ['iamjazzyfizzle', 'i', 'wish', 'i', 'got', 'to', 'watch', 'it', 'with', 'you', 'i', 'miss', 'you', 'and', 'iamlilnicki', 'how', 'was', 'the', 'premiere'] \n",
      "After Word2vec, frekansı az olan kelimeler çıkarıldı:\n",
      "Word embeddings for first sentence: (17, 120)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Word2vec, length of sentence is: {} \\n {} \".format(len(x[15]),x[15]))\n",
    "print(\"After Word2vec, frekansı az olan kelimeler çıkarıldı:\")\n",
    "print(\"Word embeddings for first sentence:\",np.array(x_features[15]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['motorcycle' '0.4432039260864258']\n",
      " ['bike' '0.43241509795188904']\n",
      " ['phone' '0.4299317002296448']\n",
      " ['room' '0.4166398048400879']\n",
      " ['convertible' '0.41147440671920776']\n",
      " ['house' '0.40856924653053284']\n",
      " ['apt' '0.407231867313385']\n",
      " ['truck' '0.40702593326568604']\n",
      " ['driveway' '0.40367114543914795']\n",
      " ['roof' '0.3988969326019287']]\n",
      "**********************************************\n",
      "[['back' '0.728092610836029']\n",
      " ['bed' '0.6311469674110413']\n",
      " ['work' '0.5888208150863647']\n",
      " ['today' '0.5094830989837646']\n",
      " ['school' '0.4937937259674072']\n",
      " ['now' '0.48729169368743896']\n",
      " ['at' '0.4853334426879883']\n",
      " ['finally' '0.4852288067340851']\n",
      " ['off' '0.48489850759506226']\n",
      " ['tomorrow' '0.4825339615345001']]\n",
      "**********************************************\n",
      "Similarity between car and home is : 0.27421873807907104\n"
     ]
    }
   ],
   "source": [
    "word1 = \"car\"\n",
    "word2 = \"home\"\n",
    "\n",
    "print(np.array(model.wv.most_similar(positive=word1)))\n",
    "print(\"**********************************************\")\n",
    "print(np.array(model.wv.most_similar(positive=word2)))\n",
    "print(\"**********************************************\")\n",
    "print(\"Similarity between {} and {} is : {}\".format(word1,word2,model.wv.similarity(word1,word2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding is done.\n"
     ]
    }
   ],
   "source": [
    "def paddedFeatures(features, target_dimension, position):\n",
    "    import numpy as np\n",
    "    padded_features = []\n",
    "    \n",
    "    if position == \"right\":\n",
    "        for sample in features:\n",
    "            zeros = np.zeros((target_dimension-sample.shape[0],sample.shape[1]))\n",
    "            padded_sample = np.concatenate((sample,zeros),axis=0)\n",
    "            padded_features.append(padded_sample)\n",
    "             \n",
    "    if position == \"left\":\n",
    "        for sample in features:\n",
    "            zeros = np.zeros((target_dimension-sample.shape[0],sample.shape[1]))\n",
    "            padded_sample = np.concatenate((zeros,sample),axis=0)\n",
    "            padded_features.append(padded_sample)\n",
    "            \n",
    "    print(\"Padding is done.\")\n",
    "            \n",
    "    return padded_features\n",
    "\n",
    "x_features = paddedFeatures(x_features, target_dimension= max_tokens_lengths, position = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings for a sentence after zero-padding: (60, 120)\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embeddings for a sentence after zero-padding:\",np.array(x_features[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Data Split Train and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y, test_size= split_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111655, 60, 120) (111655, 2)\n",
      "(Number of sentence),(Number of tokens in each sample), (embedding size for each token)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape)\n",
    "print(\"(Number of sentence),(Number of tokens in each sample), (embedding size for each token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\PROGRAM FILES\\Anaconda\\envs\\staj_projesi\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units = 100, dropout=0.2, recurrent_dropout=0.2,return_sequences=False))\n",
    "#model.add(LSTM(units = 100, dropout=0.2, recurrent_dropout=0.2,return_sequences=False))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\PROGRAM FILES\\Anaconda\\envs\\staj_projesi\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From E:\\PROGRAM FILES\\Anaconda\\envs\\staj_projesi\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/70\n",
      " - 178s - loss: 0.6720 - acc: 0.5622\n",
      "Epoch 2/70\n",
      " - 165s - loss: 0.6428 - acc: 0.6416\n",
      "Epoch 3/70\n",
      " - 169s - loss: 0.6641 - acc: 0.5720\n",
      "Epoch 4/70\n",
      " - 167s - loss: 0.6609 - acc: 0.5831\n",
      "Epoch 5/70\n",
      " - 167s - loss: 0.6681 - acc: 0.5654\n",
      "Epoch 6/70\n",
      " - 168s - loss: 0.6596 - acc: 0.5903\n",
      "Epoch 7/70\n",
      " - 173s - loss: 0.6622 - acc: 0.5902\n",
      "Epoch 8/70\n",
      " - 174s - loss: 0.6590 - acc: 0.6064\n",
      "Epoch 9/70\n",
      " - 170s - loss: 0.6594 - acc: 0.6008\n",
      "Epoch 10/70\n",
      " - 169s - loss: 0.6634 - acc: 0.5955\n",
      "Epoch 11/70\n",
      " - 173s - loss: 0.6510 - acc: 0.6246\n",
      "Epoch 12/70\n",
      " - 171s - loss: 0.5354 - acc: 0.7355\n",
      "Epoch 13/70\n",
      " - 166s - loss: 0.4837 - acc: 0.7678\n",
      "Epoch 14/70\n",
      " - 171s - loss: 0.4621 - acc: 0.7797\n",
      "Epoch 15/70\n",
      " - 166s - loss: 0.4457 - acc: 0.7900\n",
      "Epoch 16/70\n",
      " - 162s - loss: 0.4345 - acc: 0.7962\n",
      "Epoch 17/70\n",
      " - 163s - loss: 0.4258 - acc: 0.8008\n",
      "Epoch 18/70\n",
      " - 163s - loss: 0.4189 - acc: 0.8050\n",
      "Epoch 19/70\n",
      " - 167s - loss: 0.4118 - acc: 0.8094\n",
      "Epoch 20/70\n",
      " - 171s - loss: 0.4073 - acc: 0.8120\n",
      "Epoch 21/70\n",
      " - 171s - loss: 0.4026 - acc: 0.8138\n",
      "Epoch 22/70\n",
      " - 173s - loss: 0.3982 - acc: 0.8166\n",
      "Epoch 23/70\n",
      " - 179s - loss: 0.3944 - acc: 0.8202\n",
      "Epoch 24/70\n",
      " - 170s - loss: 0.3899 - acc: 0.8203\n",
      "Epoch 25/70\n",
      " - 167s - loss: 0.3876 - acc: 0.8225\n",
      "Epoch 26/70\n",
      " - 166s - loss: 0.3861 - acc: 0.8244\n",
      "Epoch 27/70\n",
      " - 164s - loss: 0.3816 - acc: 0.8254\n",
      "Epoch 28/70\n",
      " - 167s - loss: 0.3809 - acc: 0.8270\n",
      "Epoch 29/70\n",
      " - 184s - loss: 0.3804 - acc: 0.8271\n",
      "Epoch 30/70\n",
      " - 176s - loss: 0.3761 - acc: 0.8299\n",
      "Epoch 31/70\n",
      " - 179s - loss: 0.3760 - acc: 0.8301\n",
      "Epoch 32/70\n",
      " - 175s - loss: 0.3753 - acc: 0.8307\n",
      "Epoch 33/70\n",
      " - 164s - loss: 0.3731 - acc: 0.8304\n",
      "Epoch 34/70\n",
      " - 161s - loss: 0.3721 - acc: 0.8304\n",
      "Epoch 35/70\n",
      " - 165s - loss: 0.3706 - acc: 0.8314\n",
      "Epoch 36/70\n",
      " - 161s - loss: 0.3690 - acc: 0.8336\n",
      "Epoch 37/70\n",
      " - 163s - loss: 0.3680 - acc: 0.8335\n",
      "Epoch 38/70\n",
      " - 164s - loss: 0.3680 - acc: 0.8334\n",
      "Epoch 39/70\n",
      " - 166s - loss: 0.3676 - acc: 0.8347\n",
      "Epoch 40/70\n",
      " - 165s - loss: 0.3634 - acc: 0.8367\n",
      "Epoch 41/70\n",
      " - 168s - loss: 0.3654 - acc: 0.8344\n",
      "Epoch 42/70\n",
      " - 173s - loss: 0.3642 - acc: 0.8363\n",
      "Epoch 43/70\n",
      " - 166s - loss: 0.3612 - acc: 0.8366\n",
      "Epoch 44/70\n",
      " - 164s - loss: 0.3610 - acc: 0.8373\n",
      "Epoch 45/70\n",
      " - 163s - loss: 0.3617 - acc: 0.8370\n",
      "Epoch 46/70\n",
      " - 163s - loss: 0.3621 - acc: 0.8359\n",
      "Epoch 47/70\n",
      " - 187s - loss: 0.3589 - acc: 0.8395\n",
      "Epoch 48/70\n",
      " - 168s - loss: 0.3600 - acc: 0.8383\n",
      "Epoch 49/70\n",
      " - 168s - loss: 0.3585 - acc: 0.8392\n",
      "Epoch 50/70\n",
      " - 176s - loss: 0.3589 - acc: 0.8391\n",
      "Epoch 51/70\n",
      " - 162s - loss: 0.3576 - acc: 0.8383\n",
      "Epoch 52/70\n",
      " - 166s - loss: 0.3572 - acc: 0.8388\n",
      "Epoch 53/70\n",
      " - 165s - loss: 0.3581 - acc: 0.8386\n",
      "Epoch 54/70\n",
      " - 165s - loss: 0.3565 - acc: 0.8399\n",
      "Epoch 55/70\n",
      " - 167s - loss: 0.3564 - acc: 0.8403\n",
      "Epoch 56/70\n",
      " - 169s - loss: 0.3549 - acc: 0.8396\n",
      "Epoch 57/70\n",
      " - 170s - loss: 0.3559 - acc: 0.8397\n",
      "Epoch 58/70\n",
      " - 173s - loss: 0.3553 - acc: 0.8404\n",
      "Epoch 59/70\n",
      " - 173s - loss: 0.3531 - acc: 0.8415\n",
      "Epoch 60/70\n",
      " - 172s - loss: 0.3548 - acc: 0.8414\n",
      "Epoch 61/70\n",
      " - 172s - loss: 0.3536 - acc: 0.8414\n",
      "Epoch 62/70\n",
      " - 173s - loss: 0.3522 - acc: 0.8426\n",
      "Epoch 63/70\n",
      " - 175s - loss: 0.3538 - acc: 0.8418\n",
      "Epoch 64/70\n",
      " - 174s - loss: 0.3532 - acc: 0.8420\n",
      "Epoch 65/70\n",
      " - 175s - loss: 0.3520 - acc: 0.8428\n",
      "Epoch 66/70\n",
      " - 177s - loss: 0.3522 - acc: 0.8425\n",
      "Epoch 67/70\n",
      " - 180s - loss: 0.3521 - acc: 0.8418\n",
      "Epoch 68/70\n",
      " - 180s - loss: 0.3520 - acc: 0.8419\n",
      "Epoch 69/70\n",
      " - 184s - loss: 0.3505 - acc: 0.8432\n",
      "Epoch 70/70\n",
      " - 177s - loss: 0.3532 - acc: 0.8417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bd6bbf6708>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(x_train, y_train, epochs = 70, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.43\n",
      "acc: 0.81\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(x_test, y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch 1/50\\n - 54s - loss: 0.6461 - acc: 0.6331\\nEpoch 2/50\\n - 53s - loss: 0.6677 - acc: 0.6044\\nEpoch 3/50\\n - 56s - loss: 0.6652 - acc: 0.5912\\nEpoch 4/50\\n - 58s - loss: 0.6648 - acc: 0.5905\\nEpoch 5/50\\n - 58s - loss: 0.6646 - acc: 0.5882\\nEpoch 6/50'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
