{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "filename= \"tweet.csv\"\n",
    "all_data = pd.read_csv(filename, encoding = \"ISO-8859-1\", header = None)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data X Shape:  30000\n",
      "All Data Y Shape:  30000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = all_data.loc[:, [5]].values\n",
    "y = all_data.loc[:, [0]].values\n",
    "\n",
    "x = [str(data) for data in list(x)]\n",
    "y = [int(data) for data in list(y)]\n",
    "\n",
    "number_of_data_in_each_class = 15000\n",
    "\n",
    "x_neg = x[:number_of_data_in_each_class]\n",
    "y_neg = y[:number_of_data_in_each_class]\n",
    "x_pos = x[850000:850000+number_of_data_in_each_class]\n",
    "y_pos = y[850000:850000+number_of_data_in_each_class]\n",
    "x = x_neg + x_pos\n",
    "y = y_neg + y_pos\n",
    "\n",
    "\n",
    "print(\"All Data X Shape: \",len(x))\n",
    "print(\"All Data Y Shape: \",len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: , <class 'list'> 30000\n",
      "<class 'str'>\n",
      "<class 'list'> 30000\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Text: ,\",type(x),len(x))\n",
    "print(type(x[0]))\n",
    "print(type(y),len(y))\n",
    "print(type(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization for Puncutuations with NLTK module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lexical analysis, tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. It helps to reveal semantic information of words. Also, it makes text cleaning easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization:  ['@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!']\n",
      "After tokenization:  ['@ iamjazzyfizzle I wish I got to watch it with you !! I miss you and @ iamlilnicki how was the premiere ?!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "def listToString(s):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(s)) \n",
    "\n",
    "print(\"Before tokenization: \",x[15])\n",
    "x = [listToString(WordPunctTokenizer().tokenize(sentence)) for sentence in x]\n",
    "print(\"After tokenization: \",x[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization for Words with NLTK module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization:  ['@ iamjazzyfizzle I wish I got to watch it with you !! I miss you and @ iamlilnicki how was the premiere ?!']\n",
      "After tokenization:  [ ' @ iamjazzyfizzle I wish I got to watch it with you ! ! I miss you and @ iamlilnicki how was the premiere ? ! ' ]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def listToString(s):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(s)) \n",
    "\n",
    "print(\"Before tokenization: \",x[15])\n",
    "x = [listToString(nltk.word_tokenize(sentence)) for sentence in x]\n",
    "print(\"After tokenization: \",x[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning using regular expressions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: \n",
      " [ ' @ iamjazzyfizzle I wish I got to watch it with you ! ! I miss you and @ iamlilnicki how was the premiere ? ! ' ]\n",
      "After cleaning: \n",
      "    iamjazzyfizzle i wish i got to watch it with you   i miss you and  iamlilnicki how was the premiere    \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation, digits\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #Preprocess the text in a single tweet\n",
    "    #arguments: tweet = a single tweet in form of string \n",
    "    #convert the tweet to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #convert all urls to sting \"URL\"\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #convert all @username to \"AT_USER\"\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER', tweet)\n",
    "    #correct all multiple white spaces to a single white space\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #convert \"#topic\" to just \"topic\"\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    \n",
    "    converter = str.maketrans('', '', punctuation)\n",
    "    tweet = tweet.translate(converter)\n",
    "    \n",
    "    converter = str.maketrans('', '', digits)\n",
    "    tweet = tweet.translate(converter)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "print(\"Before cleaning: \\n\",x[15])\n",
    "x = [preprocess_tweet(sentence) for sentence in x]\n",
    "print(\"After cleaning: \\n\",x[15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: , <class 'list'>\n",
      "<class 'str'>\n",
      "Output:  <class 'list'> 30000\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: ,\",type(x))\n",
    "print(type(x[0]))\n",
    "print(\"Output: \",type(y),len(y))\n",
    "print(type(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Multi-label Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the polarity of the tweet (0 = negative,  4 = positive)\n",
    "num_neg = []\n",
    "num_pos = []\n",
    "for i in range(len(y)):\n",
    " \n",
    "    if y[i]==0: #negative\n",
    "        y[i] = [1,0]\n",
    "        num_neg.append((i,\"negative\"))\n",
    "    elif y[i]==4: #positive\n",
    "        y[i] = [0,1]\n",
    "        num_pos.append((i,\"positive\"))\n",
    "    else:\n",
    "        print(\"Hata\")\n",
    "\n",
    "x = [sentence.split() for sentence in x] # list in list structure because of Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: , <class 'list'> 30000\n",
      "<class 'list'>\n",
      "Output:  <class 'list'> 30000\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: ,\",type(x),len(x))\n",
    "print(type(x[0]))\n",
    "print(\"Output: \",type(y),len(y))\n",
    "print(type(y[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "min_count = 10\n",
    "window = 5\n",
    "output_size = 40\n",
    "feature_epoch = 70\n",
    "model_name = \"1\"\n",
    "\n",
    "#padding max token_size\n",
    "target_dimension = 33\n",
    "\n",
    "#train-test split\n",
    "split_ratio = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies:\n",
      "**********************************************\n",
      " ['http', 'twitpic', 'com', 'awww', 'that', 's', 'a', 'bummer', 'you', 'got', 'david', 'of', 'third', 'day', 'to', 'do', 'it', 'd', 'is', 'upset', 'he', 'can', 't', 'update', 'his', 'facebook', 'by', 'and', 'might', 'cry', 'as', 'result', 'school', 'today', 'also', 'blah', 'i', 'many', 'times', 'for', 'the', 'ball', 'managed', 'save', 'rest', 'go', 'out', 'my', 'whole', 'body', 'feels', 'itchy', 'like', 'its', 'on', 'fire', 'no', 'not', 'at', 'all', 'm', 'mad', 'why', 'am', 'here', 'because', 'see', 'over', 'there', 'crew', 'need', 'hug', 'hey', 'long', 'time', 'yes', 'bit', 'only', 'lol', 'fine', 'thanks', 'how', 'nope', 'they', 'didn', 'have', 'me', 'spring', 'break', 'in', 'plain', 'city', 'snowing', 'just', 're', 'ears', 'couldn', 'bear', 'watch', 'thought', 'loss', 'was', 'idk', 'did', 'either', 'never', 'talk', 'anymore', 'would', 've', 'been', 'first', 'but', 'gun', 'really', 'though', 'wish', 'with', 'miss', 'premiere', 'death', 'scene', 'will', 'hurt', 'film', 'cut', 'now', 'about', 'file', 'taxes', 'ahh', 'ive', 'always', 'wanted', 'rent', 'love', 'oh', 'dear', 'were', 'drinking', 'forgotten', 'table', 'drinks', 'most', 'so', 'get', 'much', 'done', 'one', 'friend', 'called', 'asked', 'meet', 'her', 'mid', 'sigh', 'cake', 'this', 'week', 'going', 'had', 'class', 'tomorrow', 'hate', 'when', 'call', 'wake', 'people', 'up', 'myself', 'sleep', 'after', 'watching', 'marley', 'im', 'sad', 'ooooh', 'ok', 'won', 'again', 'meh', 'almost', 'track', 'gets', 'depressed', 'every', 'some', 'account', 'aim', 'make', 'new', 'want', 'promote', 'ride', 'may', 'b', 'sleeping', 'an', 'option', 'morning', 'work', 'afternoon', 'awe', 'too', 'eyes', 'night', 'sick', 'spent', 'hour', 'sitting', 'shower', 'cause', 'stand', 'back', 'bed', 'ill', 'tell', 'ya', 'story', 'later', 'good', 'be', 'workin', 'three', 'more', 'hours', 'sorry', 'came', 'gd', 'don', 'depressing', 'think', 'even', 'know', 'kids', 'gym', 'or', 'then', 'another', 'gon', 'na', 'fly', 'girlfriend', 'feel', 'getting', 'study', 'exam', 'reason', 'guitar', 'who', 'has', 'enough', 'heart', 'feeling', 'wan', 'still', 'jonathanrknight', 'soo', 'finally', 'missed', 'falling', 'asleep', 'heard', 'girl', 'being', 'found', 'breaks', 'family', 'yay', 'happy', 'your', 'job', 'means', 'less', 'checked', 'user', 'blackberry', 'looks', 'happening', 'are', 'ppl', 'having', 'probs', 'w', 'man', 'fave', 'top', 'wear', 'meeting', 'burnt', 'breaking', 'tea', 'before', 'plan', 'massive', 'broken', 'promise', 'tinyurl', 'via', 'www', 'waiting', 'we', 'wow', 'tons', 'replies', 'from', 'unfollow', 'friends', 'tweets', 'feed', 'lot', 'our', 'chicken', 'taking', 'put', 'vacation', 'photos', 'online', 'few', 'yrs', 'ago', 'pc', 'crashed', 'forget', 'name', 'site', 'sure', 'what', 'dont', 'trade', 'away', 'company', 'andy', 'happens', 'dallas', 'show', 'ta', 'say', 'shows', 'use', 'music', 'game', 'mmm', 'ugh', 'degrees', 'where', 'u', 'move', 'already', 'hmmm', 'random', 'glad', 'hear', 'yer', 'doing', 'well', 'ps', 'playing', 'blood', 'leaving', 'life', 'cool', 'sadly', 'gotten', 'experience', 'post', 'such', 'nice', 'bad', 'rain', 'comes', 'around', 'lost', 'pay', 'phone', 'bill', 'lmao', 'aw', 'mo', 'jobs', 'money', 'hell', 'min', 'f', 'n', 'forever', 'soon', 'agreed', 'saw', 'haha', 'dude', 'look', 'em', 'unless', 'someone', 'says', 'added', 'terrible', 'pop', 'right', 'start', 'working', 'least', 'clearly', 'ugly', 'shorts', 'black', 'business', 'socks', 'shoes', 'lucky', 'run', 'into', 'any', 'cute', 'girls', 'picnic', 'smells', 'comments', 'asap', 'charger', 'tonight', 'fml', 'arms', 'sore', 'tennis', 'wonders', 'saying', 'bye', 'ur', 'those', 'shame', 'booked', 'paid', 'mine', 'missin', 'boo', 'damn', 'useless', 'blast', 'hates', 'she', 'throat', 'worse', 'mama', 'tummy', 'hurts', 'wonder', 'if', 'anything', 'stop', 'smoking', 'fat', 'ones', 'babe', 'fam', 'thankfully', 'evil', 'laugh', 'should', 'attention', 'covered', 'photoshop', 'design', 'wednesday', 'poor', 'hills', 'pray', 'please', 'ex', 'babies', 'st', 'birthday', 'party', 'headache', 'hmm', 'enjoy', 'him', 'problems', 'things', 'find', 'little', 'puppy', 'apps', 'dogbook', 'profile', 'view', 'wana', 'steve', 'since', 'easter', 'able', 'ohh', 'actually', 'wasn', 'follow', 'nite', 'favorite', 'lose', 'missing', 'california', 'tr', 'hope', 'fast', 'yesterday', 'pain', 'fail', 'behind', 'classes', 'quot', 'house', 'remember', 'bum', 'leg', 'serious', 'their', 'laptop', 'emily', 'mommy', 'training', 'misses', 'rather', 'send', 'messages', 'than', 'rd', 'year', 'same', 'thing', 'laying', 'voice', 'sooo', 'killed', 'off', 'kutner', 'them', 'sense', 'believe', 'happened', 'grind', 'cuz', 'yeah', 'chance', 'cant', 'hanging', 'sing', 'sucks', 'aww', 'beach', 'pissed', 'radio', 'station', 'down', 'head', 'sounds', 'mention', 'crying', 'made', 'late', 'snack', 'glass', 'c', 'big', 'fan', 'wah', 'must', 'el', 'wait', 'till', 'something', 'else', 'blame', 'broke', 'seems', 'longer', 'cold', 'weather', 'take', 'turn', 'incredible', 'stuff', 'hoping', 'notice', 'told', 'said', 'bedtime', 'alive', 'tired', 'imma', 'try', 'hopefully', 'snow', 'thank', 'letting', 'direct', 'message', 'india', 'th', 'test', 'win', 'without', 'guess', 'finish', 'editing', 'page', 'probably', 'happen', 'pages', 'left', 'haven', 'read', 'amp', 'saving', 'end', 'easy', 'books', 'died', 'mom', 'cancer', 'worried', 'better', 'ended', 'breakfast', 'keep', 'waking', 'lame', 'understand', 'heroes', 'isn', 'season', 'living', 'downtown', 'fun', 'wise', 'food', 'free', 'ate', 'ass', 'coke', 'hard', 'hot', 'studying', 'star', 'sweet', 'although', 'sent', 'picked', 'pretty', 'pick', 'way', 'until', 'alone', 'mean', 'seriously', 'kinda', 'g', 'chicago', 'far', 'stuck', 'awake', 'middle', 'second', 'row', 'felt', 'gosh', 'sooooo', 'deleted', 'history', 'crazy', 'wind', 'ff', 'currently', 'grrr', 'acting', 'weird', 'ho', 'thinking', 'aren', 'full', 'songs', 'ughh', 'dvd', 'cos', 'deal', 'website', 'through', 'other', 'tips', 'swear', 'losing', 'tweeps', 'realized', 'hiding', 'staying', 'loud', 'danny', 'wasnt', 'live', 'chat', 'car', 'trip', 'soooo', 'check', 'blogspot', 'closed', 'downloading', 'album', 'come', 'these', 'days', 'woke', 'e', 'mail', 'early', 'university', 'teach', 'hill', 'making', 'yet', 'boring', 'lazy', 'buddy', 'ny', 'french', 'south', 'beautiful', 'problem', 'block', 'clock', 'difficult', 'fb', 'guy', 'dying', 'next', 'woman', 'guys', 'dog', '¿', '½s', 'shit', 'screwed', 'tonite', 'interview', 'luck', 'choose', 'help', 'dead', 'kill', 'seen', 'ds', 'hahaha', 'hrs', 'mind', 'boot', 'goin', 'library', 'doesn', 'nap', 'japanese', 'kind', 'bus', 'world', 'canada', 'supposed', 'shop', 'o', 'movie', 'feet', 'macbook', 'fell', 'crash', 'while', 'somebody', 'couldnt', 'handle', 'cat', 'heat', 'donniewahlberg', 'looking', 'thats', 'fit', 'pictures', 'ever', 'tho', 'congrats', 'totally', 'forgot', 'amazing', 'xxx', 'twitter', 'using', 'stupid', 'ughhh', 'shooting', 'outside', 'kidding', 'scared', 'ï', '½n', 'lecture', 'stress', 'become', 'very', 'useful', 'us', 'news', 'trying', 'picture', 'upload', 'red', 'sox', 'forward', 'opening', 'dunno', 'home', 'town', 'pls', 'explain', 'y', 'let', 'yr', 'old', 'child', 'walk', 'hello', 'combo', 'restaurant', 'eating', 'round', 'place', 'nd', 'bathroom', 'gay', 'lt', 'lonely', 'female', 'office', 'celebrate', 'weekend', 'nothing', 'officially', 'wont', 'write', 'band', 'lead', 'gt', 'john', 'play', 'fruit', 'doctor', 'bff', 'reminds', 'shout', 'video', 'card', 'naked', 'confused', 'jk', 'sort', 'iphone', 'form', 'past', 'change', 'wat', 'gah', 'ring', 'download', 'internet', 'fuck', 'baby', 'fucked', 'uni', 'netball', 'april', 'july', 'comin', 'jealous', 'face', 'golden', 'piece', 'fucking', 'ooh', 'iï', '½m', 'excited', 'youtube', 'reach', 'bank', 'holiday', 'agree', 'theres', 'often', 'hilarious', 'lj', 'went', 'link', 'neither', 'tweeted', 'nearly', 'posted', 'goes', 'needs', 'son', 'stay', 'client', 'boss', 'english', 'text', 'v', 'games', 'related', 'figure', 'hotel', 'etc', 'bc', 'register', 'slept', 'nighty', 'afraid', 'code', 'gone', 'needed', 'pee', 'yup', 'r', 'worst', 'schedule', 'tough', 'list', 'bar', 'hasn', 'arrived', 'dancing', 'art', 'wants', 'ah', 'bet', 'does', 'craving', 'basketball', 'noo', 'accidentally', 'management', 'assignment', 'hopes', 'doesnt', 'trouble', 'worked', 'php', 'argh', 'sleepy', 'wide', 'sold', 'watched', 'thunder', 'dreading', 'bro', 'poo', 'fight', 'stolen', 'staff', 'thoughts', 'sandra', 'cantu', 'everybody', 'michigan', 'fill', 'two', 'seeing', 'everyday', 'germany', 'closing', 'cheese', 'idea', 'children', 'stories', 'seem', 'wishing', 'walking', 'energy', 'ep', 'awesome', 'dang', 'knew', 'earlier', 'planned', 'writing', 'paper', 'course', 'access', 'halo', 'pack', 'ms', 'unfortunately', 'weeks', 'busy', 'latest', 'project', 'ha', 'sun', 'harder', 'jake', 'during', 'tom', 'tried', 'tech', 'state', 'omg', 'mouth', 'annoyed', 'easily', 'videos', 'spending', 'grandma', 'leave', 'herself', 'years', 'worth', 'alot', 'finishing', 'possible', 'ache', 'last', 'miserable', 'rachel', 'hang', 'dumb', 'sat', 'bay', 'hubby', 'thinks', 'important', 'spot', 'trek', 'matter', 'easier', 'hair', 'color', 'whoa', 'coming', 'suppose', 'wtf', 'maybe', 'couple', 'productive', 'worry', 'learned', 'words', 'ahhhh', 'movies', 'rip', 'homework', 'taken', 'scratch', 'du', 'half', 'cookies', 'listening', 'sweets', 'chi', 'plz', 'meetings', 'emails', 'email', 'pm', 'dark', 'reply', 'ages', 'men', 'both', 'nickcarter', 'nick', 'jon', 'net', 'created', 'seconds', 'search', 'stock', 'running', 'swimming', 'sucked', 'die', 'chris', 'info', 'open', 'doc', 'um', 'yucky', 'quick', 'song', 'myspace', 'dad', 'started', 'coach', 'weight', 'boyfriend', 'complete', 'idiot', 'works', 'ad', 'host', 'rolls', 'l', 'monday', 'gg', 'episode', 'keeping', 'fingers', 'crossed', 'eat', 'vegas', 'hungry', 'tweetdeck', 'once', 'hated', 'btw', 'funny', 'quickly', 'freezing', 'sayin', 'wondering', 'ed', 'strong', 'hold', 'drive', 'anyone', 'accident', 'pants', 'street', 'kicked', 'could', 'shoe', 'changed', 'gave', 'bored', 'status', 'eh', 'definitely', 'copy', 'studio', 'silly', 'anyway', 'bright', 'side', 'fish', 'apparently', 'taste', 'surprised', 'everyone', 'airport', 'sell', 'comment', 'stayed', 'strange', 'everywhere', 'saturday', 'along', 'green', 'number', 'each', 'yea', 'quiet', 'nobody', 'cooking', 'okay', 'talked', 'nature', 'zone', 'fair', 'tweet', 'everything', 'tempted', 'chocolate', 'mocha', 'keeps', 'ye', 'worn', 'dreams', 'wall', 'lights', 'll', 'sister', 'nicolerichie', 'uh', 'god', 'ruined', 'belly', 'button', 'makes', 'cheap', 'buying', 'fall', 'nose', 'negative', 'vote', 'nasty', 'due', 'apple', 'dammit', 'kings', 'stopped', 'quiz', 'others', 'raw', 'ish', 'prayers', 'real', 'twilight', 'ddlovato', 'giant', 'famous', 'book', 'boston', 'public', 'garden', 'however', 'heading', 'install', 'bloody', 'garage', 'park', 'popular', 'reading', 'struggling', 'continue', 'flowers', 'camera', 'liking', 'used', 'pity', 'blocked', 'china', 'vids', 'best', 'which', 'cousins', 'daddy', 'brother', 'angel', 'trust', 'considering', 'grr', 'date', 'bottle', 'cats', 'crap', 'horrible', 'rock', 'tuesday', 'britney', 'ap', 'cook', 'took', 'ebay', 'takes', 'presentation', 'slides', 'starts', 'straight', 'sound', 'fav', 'click', 'spider', 'known', 'turned', 'hd', 'support', 'app', 'store', 'swollen', 'allergic', 'thursday', 'tree', 'floor', 'runs', 'comp', 'small', 'print', 'wasted', 'scary', 'finding', 'ready', 'bread', 'dollar', 'toast', 'nicely', 'calling', 'balls', 'computer', 'listen', 'xbox', 'meant', 'camp', 'friday', 'exactly', 'mornings', 'jesus', 'dress', 'shopping', 'mac', 'faster', 'pleasant', 'mum', 'gettin', 'wen', 'somewhere', 'close', 'marry', 'cried', 'matthew', 'absolutely', 'filled', 'true', 'indian', 'heidimontag', 'cam', 'beat', 'knight', 'hun', 'difference', 'tumblr', 'wearing', '½', 'isnt', 'word', 'dance', 'team', 'anywhere', 'goodnight', 'tweeters', 'bills', 'episodes', 'bunch', 'uk', 'moment', 'learning', 'brian', 'replied', 'network', 'catch', 'thru', 'dry', 'set', 'ground', 'weren', 'becoming', 'cross', 'lauren', 'loves', 'dropping', 'pre', 'mood', 'minutes', 'nightmare', 'york', 'tongue', 'lots', 'nz', 'matt', 'disappointed', 'mostly', 'ending', 'names', 'folks', 'yawn', 'gossip', 'repeat', 'tax', 'return', 'dream', 'cream', 'eye', 'different', 'wet', 'umm', 'plus', 'flu', 'outta', 'water', 'minute', 'earthquake', 'italy', 'stressed', 'server', 'player', 'ankle', 'dr', 'manage', 'trash', 'empty', 'ipod', 'buy', 'coz', 'shouldn', 'sex', 'careful', 'cell', 'finished', 'stephenkruiser', 'ran', 'honey', 'wrote', 'hits', 'moony', 'stomach', 'counting', 'visit', 'care', 'blues', 'room', 'itunes', 'pics', 'step', 'knows', 'loved', 'hugs', 'mouse', 'white', 'vs', 'bah', 'official', 'month', 'knowing', 'coachella', 'boooo', 'ahead', 'chill', 'egg', 'everytime', 'exist', 'jus', 'wishes', 'tour', 'dawn', 'great', 'ie', 'planet', 'abt', 'sarah', 'centre', 'point', 'guessing', 'train', 'four', 'cash', 'arent', 'delicious', 'paris', 'milk', 'europe', 'drink', 'finals', 'none', 'custom', 'square', 'background', 'la', 'talking', 'plans', 'ordered', 'instead', 'hit', 'donnie', 'stats', 'nah', 'received', 'feelin', 'decision', 'part', 'warm', 'march', 'gross', 'ma', 'follower', 'whether', 'burned', 'north', 'aint', 'twitters', 'vid', 'single', 'gives', 'pulled', 'invite', 'healthy', 'session', 'skip', 'hella', 'procrastinating', 'clothes', 'bestie', 'double', 'fans', 'suck', 'wrong', 'decided', 'rules', 'market', 'boy', 'sydney', 'joke', 'except', 'spell', 'against', 'sam', 'spend', 'fab', 'strawberry', 'insomnia', 'screen', 'corner', 'lines', 'confusing', 'finger', 'traffic', 'cuddle', 'software', 'updates', 'telling', 'diet', 'pancakes', 'hating', 'cruel', 'reviews', 'google', 'speak', 'clean', 'bottom', 'moving', 'share', 'caught', 'visiting', 'winter', 'desk', 'dvds', 'ear', 'infection', 'sunday', 'awhile', 'yo', 'alarm', 'wings', 'fed', 'bummed', 'ladies', 'create', 'goodness', 'travel', 'france', 'miles', 'review', 'cup', 'coffee', 'dentist', 'drunk', 'realised', 'huge', 'looked', 'nights', 'certain', 'crappy', 'law', 'driving', 'packing', 'painting', 'cleaning', 'appreciated', 'beer', 'under', 'kitchen', 'tests', 'failed', 'victoria', 'followed', 'anybody', 'afford', 'pet', 'mia', 'p', 'door', 'large', 'usually', 'character', 'austin', 'q', 'lesson', 'mins', 'shut', 'essay', 'tmrw', 'per', 'usual', 'noooooo', 'played', 'hand', 'calls', 'crack', 'trees', 'ouch', 'term', 'eve', 'sites', 'together', 'checking', 'ideas', 'nowhere', 'log', 'plurk', 'mon', 'sean', 'league', 'own', 'ice', 'grand', 'grass', 'bike', 'loads', 'dropped', 'london', 'raining', 'bless', 'completely', 'contacts', 'invited', 'starting', 'lately', 'contact', 'pro', 'flash', 'lovely', 'order', 'vitamin', 'posts', 'properly', 'italian', 'fact', 'between', 'quite', 'fake', 'cooked', 'baseball', 'yankees', 'exams', 'suddenly', 'journey', 'lookin', 'darn', 'allergies', 'ca', 'tv', 'html', 'cable', 'area', 'morn', 'hadn', 'legs', 'hi', 'fixed', 'king', 'kitty', 'butt', 'conversation', 'interesting', 'sisters', 'brilliant', 'bring', 'record', 'rt', 'kimkardashian', 'surprise', 'kalpenn', 'helping', 'papers', 'seat', 'mobile', 'concentrate', 'enter', 'waste', 'migraine', 'shoot', 'turning', 'line', 'didnt', 'zero', 'reader', 'decent', 'exchange', 'rate', 'ohhh', 'somehow', 'group', 'campus', 'h', 'texas', 'wit', 'size', 'painful', 'tht', 'annoying', 'def', 'kno', 'mileycyrus', 'learn', 'noon', 'innocent', 'selling', 'web', 'ben', 'total', 'cost', 'lay', 'ily', 'yours', 'application', 'add', 'talent', 'msu', 'tuesdays', 'killing', 'glasses', 'arm', 'meat', 'slow', 'ask', 'sis', 'skype', 'laughing', 'wanting', 'org', 'spoiled', 'coughing', 'shitty', 'turns', 'hurting', 'yeh', 'holidays', 'woot', 'hearing', 'attend', 'notes', 'decide', 'spammers', 'followers', 'release', 'classic', 'bags', 'searching', 'fyi', 'yahoo', 'edit', 'bio', 'co', 'answer', 'bt', 'starving', 'foot', 'smart', 'given', 'teh', 'theory', 'woken', 'van', 'main', 'road', 'rofl', 'hows', 'hannah', 'disappointing', 'alright', 'sports', 'planning', 'evening', 'joined', 'named', 'rainy', 'til', 'wk', 'extremely', 'summer', 'password', 'give', 'spam', 'extra', 'hospital', 'en', 'sorting', 'boohoo', 'ellen', 'tooo', 'da', 'fix', 'avatar', 'box', 'tiny', 'appointment', 'addicted', 'dressed', 'self', 'slowly', 'focus', 'gas', 'note', 'machine', 'closer', 'donï', '½t', 'slightly', 'exciting', 'characters', 'pass', 'effing', 'points', 'shes', 'parents', 'amsterdam', 'attack', 'lily', 'growing', 'hint', 'person', 'pic', 'signed', 'shirt', 'inside', 'bag', 'mother', 'cough', 'fantastic', 'gutted', 'windy', 'concert', 'expensive', 'arrive', 'exercise', 'paying', 'kisses', 'excellent', 'burn', 'motivation', 'nooo', 'west', 'coast', 'badly', 'battery', 'australia', 'driver', 'atleast', 'ac', 'dc', 'reality', 'xoxo', 'doubt', 'beauty', 'hehehe', 'heh', 'receive', 'digital', 'fellow', 'special', 'bday', 'id', 'whats', 'meds', 'blip', 'fm', 'normal', 'tweeting', 'tunes', 'chick', 'process', 'canceled', 'plane', 'tickets', 'delete', 'awful', 'imagine', 'cousin', 'following', 'bleh', 'bought', 'michael', 'x', 'holy', 'original', 'revision', 'final', 'mark', 'nails', 'passed', 'hip', 'bf', 'ch', 'ng', 'moved', 'oops', 'cd', 'oooh', 'heads', 'high', 'dinner', 'dm', 'smile', 'brings', 'memories', 'issue', 'deep', 'ticket', 'hm', 'freakin', 'current', 'challenge', 'international', 'community', 'question', 'nl', 'txt', 'lmfao', 'xxxx', 'hehe', 'gb', 'upgrade', 'humor', 'super', 'fault', 'mtv', 'gig', 'begins', 'simple', 'wooo', 'knee', 'kick', 'blue', 'havent', 'mary', 'hoo', 'light', 'headed', 'major', 'field', 'service', 'somethin', 'windows', 'issues', 'months', 'non', 'peace', 'basically', 'mornin', 'lie', 'likes', 'type', 'liked', 'situation', 'angry', 'dreaming', 'key', 'someday', 'shall', 'enjoying', 'built', 'soup', 'lil', 'hahahaha', 'goodbye', 'stage', 'favourite', 'arse', 'bigger', 'remembered', 'fridge', 'edge', 'skin', 'likely', 'race', 'chrisdjmoyles', 'unable', 'perhaps', 'especially', 'admit', 'joy', 'beginning', 'brain', 'lady', 'appreciate', 'bowling', 'mate', 'rubbish', 'obviously', 'pr', 'near', 'feature', 'ow', 'bitch', 'shift', 'dougiemcfly', 'bunny', 'ly', 'moon', 'luckily', 'shot', 'tip', 'eaten', 'lunch', 'backup', 'oprah', 'tony', 'tx', 'ahaha', 'dollars', 'husband', 'wonderful', 'cop', 'creepy', 'truth', 'lab', 'report', 'hr', 'msn', 'cali', 'woo', 'seven', 'heck', 'goal', 'charge', 'june', 'ew', 'sux', 'drama', 'places', 'delayed', 'phoenix', 'san', 'diego', 'obsessed', 'fresh', 'shining', 'birds', 'singing', 'addiction', 'aus', 'sign', 'age', 'relax', 'air', 'language', 'blog', 'low', 'commercial', 'draw', 'goodmorning', 'poorly', 'melbourne', 'cereal', 'mondays', 'fool', 'brand', 'hawaii', 'updating', 'tommcfly', 'solution', 'spanish', 'sims', 'dnt', 'prefer', 'youre', 'secret', 'projects', 'revising', 'figured', 'east', 'perfectly', 'banana', 'fever', 'hands', 'math', 'magic', 'power', 'rid', 'loser', 'enjoyed', 'future', 'nt', 'safe', 'disney', 'channel', 'promised', 'wouldn', 'pa', 'dat', 'guilty', 'control', 'cheer', 'midnight', 'bout', 'root', 'noise', 'short', 'si', 'se', 'awwww', 'yourself', 'hes', 'mess', 'entire', 'response', 'accounts', 'twitterberry', 'window', 'chillin', 'sugar', 'rice', 'sometimes', 'sales', 'development', 'laundry', 'proud', 'tattoo', 'messed', 'rudd', 'au', 'playin', 'funeral', 'cc', 'whore', 'oz', 'jokes', 'realize', 'queen', 'forced', 'sooner', 'gorgeous', 'roll', 'booo', 'smoke', 'impossible', 'files', 'neck', 'le', 'five', 'audio', 'hurry', 'tune', 'piano', 'ends', 'chip', 'wedding', 'shots', 'twit', 'ads', 'regular', 'tommorow', 'hooray', 'schofe', 'watchin', 'bits', 'pizza', 'itï', 'thurs', 'level', 'updated', 'sebbypeek', 'himself', 'hole', 'sale', 'christmas', 'fixing', 'role', 'banned', 'latte', 'wisdom', 'teeth', 'price', 'thankyou', 'throw', 'obama', 'sunshine', 'yep', 'bite', 'sf', 'bath', 'appt', 'hitting', 'rocks', 'sunny', 'lets', 'wash', 'flight', 'grrrr', 'teacher', 'graduation', 'common', 'george', 'ahhh', 'nooooo', 'allowed', 'students', 'yum', 'xd', 'lived', 'hat', 'xx', 'aplusk', 'several', 'hangover', 'soft', 'simply', 'domain', 'anytime', 'grey', 'grumpy', 'pink', 'rustyrockets', 'bedroom', 'opened', 'standing', 'atm', 'sky', 'shoulder', 'questions', 'gold', 'k', 'choice', 'cutest', 'miley', 'chinese', 'stars', 'huh', 'fear', 'eminem', 'error', 'yall', 'alas', 'dj', 'spotify', 'charging', 'towards', 'gud', 'version', 'nyc', 'thnx', 'orange', 'starbucks', 'college', 'sushi', 'photo', 'xp', 'island', 'virus', 'system', 'festival', 'de', 'itself', 'pool', 'mix', 'tiring', 'beta', 'mistake', 'quote', 'fashion', 'war', 'faces', 'hw', 'freaking', 'ummm', 'candy', 'progress', 'hero', 'available', 'loose', 'cheers', 'country', 'tryin', 'wife', 'gunna', 'blessed', 'nor', 'article', 'epic', 'nervous', 'tweetie', 'mike', 'bob', 'gf', 'pulling', 'pair', 'oo', 'ups', 'uploaded', 'onto', 'load', 'jonas', 'brothers', 'patch', 'count', 'stream', 'gfalcone', 'picking', 'rough', 'podcast', 'doctors', 'results', 'vet', 'science', 'space', 'firefox', 'dirty', 'loving', 'daughter', 'across', 'joe', 'suffering', 'fest', 'multiple', 'di', 'sa', 'jason', 'flat', 'wine', 'boys', 'jersey', 'mode', 'kept', 'apply', 'competition', 'devil', 'bacon', 'sandwich', 'lime', 'opinion', 'jeans', 'inspiration', 'listened', 'memory', 'dogs', 'church', 'flying', 'dan', 'ain', 'un', 'worries', 'research', 'bugs', 'blow', 'ol', 'rite', 'quit', 'breath', 'daily', 'bucks', 'bug', 'pounds', 'knees', 'comic', 'credit', 'ashley', 'brought', 'consider', 'bb', 'mrs', 'respond', 'vista', 'fuckin', 'tooth', 'spa', 'feedback', 'simon', 'contest', 'darling', 'perfect', 'changing', 'mr', 'sony', 'tight', 'lolz', 'josh', 'yummy', 'literally', 'animal', 'wordpress', 'otherwise', 'coursework', 'cookie', 'ubuntu', 'ray', 'society', 'twittering', 'bowl', 'holding', '½i', 'including', 'links', 'dollhouse', 'rehearsal', 'flickr', 'private', 'kid', 'welcome', 'offer', 'camping', 'jump', 'thanx', 'deserve', 'putting', 'personal', 'stores', 'usa', 'dave', 'failing', 'xo', 'playlist', 'maths', 'fighting', 'shud', 'butter', 'limit', 'ooo', 'geek', 'babysitting', 'excuse', 'exhausted', 'lake', 'shaundiviney', 'action', 'services', 'blew', 'envy', 'unlimited', 'sometime', 'england', 'touch', 'giving', 'tells', 'practice', 'front', 'congratulations', 'smell', 'lyrics', 'nxt', 'request', 'replace', 'wheel', 'advice', 'american', 'seemed', 'shops', 'sit', 'balcony', 'swim', 'mmmm', 'impressed', 'social', 'surgery', 'style', 'rule', 'performance', 'twice', 'connection', 'daniel', 'dannymcfly', 'speech', 'youth', 'virtual', 'chilling', 'biggest', 'building', 'honest', 'idol', 'fox', 'britneyspears', 'born', 'ftw', 'relaxing', 'mall', 'indeed', 'recently', 'paint', 'riding', 'pressure', 'conference', 'pat', 'cloudy', 'tap', 'bbc', 'entertainment', 'creative', 'interested', 'realise', 'preparing', 'peeps', 'expect', 'certainly', 'pub', 'license', 'cancelled', 'makin', 'barely', 'catching', 'url', 'tan', 'bï', 'mï', 'dig', 'changes', 'discovered', 'adding', 'matters', 'hav', 'servers', 'kicking', 'dies', 'al', 'vintage', 'noticed', 'patient', 'bradiewebbstack', 'logo', 'offline', 'kitten', 'taylor', 'guest', 'product', 'shine', 'yellow', 'prob', 'young', 'sack', 'chatting', 'despite', 'met', 'tears', 'sending', 'marathon', 'miami', 'cares', 'surely', 'land', 'quality', 'cards', 'build', 'older', 'weekends', 'beyond', 'hardly', 'male', 'program', 'amount', 'collection', 'showing', 'sexy', 'swine', 'asking', 'earth', 'lemon', 'juice', 'yuck', 'possibly', 'havin', 'whatever', 'local', 'depends', 'attempting', 'display', 'testing', 'andrew', 'suggestion', 'tomfelton', 'proper', 'present', 'smiling', 'swift', 'cover', 'users', 'washing', 'uncle', 'cars', 'ways', 'walked', 'boots', 'golf', 'cure', 'hunting', 'panda', 'football', 'fancy', 'speaking', 'cupcakes', 'drop', 'women', 'heya', 'storm', 'hahah', 'requests', 'expected', 'tbh', 'pleasure', 'leaves', 'inspired', 'section', 'comedy', 'grow', 'kevin', 'curious', 'talkin', 'wouldnt', 'workout', 'bbq', 'mile', 'fully', 'apartment', 'burning', 'chelsea', 'wee', 'hah', 'mini', 'musical', 'typing', 'stressful', 'tha', 'sum', 'stephenfry', 'eggs', 'kate', 'insurance', 'houston', 'harry', 'content', 'join', 'posting', 'match', 'ohio', 'tastes', 'salad', 'minor', 'paul', 'taco', 'interest', 'lord', 'gmail', 'jack', 'freedom', 'retweet', 'cakes', 'installed', 'katie', 'desperate', 'faith', 'based', 'desktop', 'thick', 'lbs', 'cs', 'parties', 'umbrella', 'whilst', 'smiley', 'grab', 'student', 'iamjonathancook', 'james', 'bringing', 'las', 'couch', 'lives', 'series', 'brown', 'shirts', 'sweetheart', 'stone', 'general', 'mentioned', 'upstairs', 'goood', 'images', 'hide', 'kiss', 'colour', 'sweden', 'father', 'title', 'recommend', 'grocery', 'boat', 'philly', 'locked', 'flights', 'sniff', 'switch', 'ryan', 'heavy', 'pull', 'prom', 'stick', 'hockey', 'twitterverse', 'cow', 'er', 'tis', 'pie', 'nicer', 'lack', 'phil', 'board', 'castle', 'soccer', 'six', 'odd', 'model', 'glorious', 'court', 'duty', 'chilly', 'event', 'setting', 'similar', 'mi', 'club', 'perezhilton', 'cast', 'fabulous', 'speed', 'thinkin', 'previous', 'case', 'whenever', 'fi', 'drove', 'positive', 'heels', 'florida', 'flooded', 'doin', 'recording', 'lessons', 'clouds', 'brazil', 'chuck', 'blogs', 'weekly', 'woohoo', 'nuts', 'bird', 'apart', 'score', 'media', 'health', 'center', 'woop', 'rob', 'demi', 'ka', 'honestly', 'phones', 'yey', 'married', 'expecting', 'scream', 'comfy', 'surfing', 'recovering', 'details', 'sorted', 'ringtones', 'convince', 'sheets', 'j', 'ing', 'ridiculous', 'adorable', 'wii', 'wt', 'yard', 'sweetie', 'homie', 'saturdays', 'petewentz', 'ii', 'push', 'clear', 'downloaded', 'creating', 'sharing', 'pirates', 'guide', 'helps', 'soul', 'toe', 'ipl', 'johncmayer', 'semester', 'morrow', 'belong', 'success', 'anyways', 'momma', 'motion', 'august', 'singer', 'selenagomez', 'susan', 'twins', 'aunt', 'kim', 'luv', 'photography', 'twitterland', 'asot', 'iamdiddy', 'haircut', 'treat', 'bride', 'tix', 'jay', 'wars', 'officialashleyg', 'aha', 'jb', 'stoked', 'wild', 'whose', 'clients', 'rich', 'hung', 'realhughjackman', 'million', 'followfriday', 'theme', 'mcfly', 'purpose', 'hop', 'truly', 'lovin', 'vancouver', 'jonasbrothers', 'uploading', 'address', 'caps', 'dare', 'pocket', 'thx', 'adventure', 'wins', 'thee', 'adam', 'heyy', 'yah', 'blogging', 'dawnrichard', 'dannywood', 'oracle', 'wolverine', 'sir', 'blessings', 'hugh', 'origins', 'ccr', 'yoga', 'twiddeo', 'liquidwings', 'regs', 'billing', 'kardashians', 'joelmadden', 'debbas', 'pfv', 'tink', 'goodnite', 'jordanknight', 'grease', 'cbcr']\n",
      "Vocab. length:  2869\n",
      "Time to build vocab: 0.0 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:05<00:00, 5420.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing is done. 0.09 mins\n",
      "Time to train the model: 0.25 mins\n",
      "Maximum token length in each sample:  50\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def Word2vecFeatureExtraction(input, y, min_count,window, output_size, epoch, model_name):\n",
    "    from gensim.models import Word2Vec\n",
    "    import multiprocessing\n",
    "    from time import time  # To time our operations\n",
    "    \n",
    "    #create model\n",
    "    model = Word2Vec(min_count = min_count,window=window,size=output_size,sample=6e-5,alpha=0.03,min_alpha=0.0007,negative=20,workers=4,compute_loss=True)\n",
    "    t = time()\n",
    "    model.build_vocab(input, progress_per=10000)\n",
    "    vocab = list(model.wv.vocab)\n",
    "    print(\"Vocabularies:\\n**********************************************\\n\",vocab)\n",
    "    print(\"Vocab. length: \",len(vocab))\n",
    "    print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    \n",
    "\n",
    "    # remove out-of-vocabulary words\n",
    "    t = time()\n",
    "    all_data = []\n",
    "    idx = 0\n",
    "    for sentence in tqdm(input):\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                new_sentence.append(word)\n",
    "        if len(new_sentence)==0:\n",
    "            y.pop(idx)\n",
    "            idx = idx + 1\n",
    "        else:\n",
    "            all_data.append(new_sentence)\n",
    "            idx = idx + 1\n",
    "            \n",
    "    print(\"Removing is done. {} mins\".format(round((time() - t) / 60, 2)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #train the Word2vec model\n",
    "    t = time()\n",
    "    model.train(all_data, total_examples = model.corpus_count, epochs = epoch, report_delay=1)\n",
    "    model.save(\"epoch_\" + str(epoch) + \"_output_size_\"+str(output_size)+\"_\"+ model_name + \".bin\")\n",
    "    print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    \n",
    "    \n",
    "    tokens_lengths = [len(model.wv[value]) for value in all_data]\n",
    "    print(\"Maximum token length in each sample: \", max(tokens_lengths))\n",
    "    \n",
    "    features = [model.wv[value] for value in all_data]\n",
    "    #model = Word2Vec.load(model_name)\n",
    "    \n",
    "    return features,y,max(tokens_lengths), model\n",
    "\n",
    "\n",
    "x_features,y,max_tokens_lengths, model = Word2vecFeatureExtraction(x, y, min_count, window, output_size, feature_epoch, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Word2vec, length of sentence is: 19 \n",
      " ['iamjazzyfizzle', 'i', 'wish', 'i', 'got', 'to', 'watch', 'it', 'with', 'you', 'i', 'miss', 'you', 'and', 'iamlilnicki', 'how', 'was', 'the', 'premiere'] \n",
      "After Word2vec, frekansı az olan kelimeler çıkarıldı:\n",
      "Word embeddings for first sentence: (17, 40)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Word2vec, length of sentence is: {} \\n {} \".format(len(x[15]),x[15]))\n",
    "print(\"After Word2vec, frekansı az olan kelimeler çıkarıldı:\")\n",
    "print(\"Word embeddings for first sentence:\",np.array(x_features[15]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['accident' '0.5716012120246887']\n",
      " ['machine' '0.5589753985404968']\n",
      " ['dropped' '0.5571492910385132']\n",
      " ['shops' '0.5541484355926514']\n",
      " ['van' '0.5377576351165771']\n",
      " ['into' '0.5316316485404968']\n",
      " ['bathroom' '0.5268046855926514']\n",
      " ['garage' '0.5234856605529785']\n",
      " ['ran' '0.5125377178192139']\n",
      " ['opened' '0.5107675790786743']]\n",
      "**********************************************\n",
      "[['back' '0.6442150473594666']\n",
      " ['lunch' '0.6284584403038025']\n",
      " ['work' '0.6224349737167358']\n",
      " ['leaving' '0.6159682273864746']\n",
      " ['tired' '0.6112692952156067']\n",
      " ['babysitting' '0.6034722924232483']\n",
      " ['today' '0.6004750728607178']\n",
      " ['sisters' '0.6003066301345825']\n",
      " ['house' '0.5973416566848755']\n",
      " ['breakfast' '0.5951350331306458']]\n",
      "**********************************************\n",
      "Similarity between car and home is : 0.36809277534484863\n"
     ]
    }
   ],
   "source": [
    "word1 = \"car\"\n",
    "word2 = \"home\"\n",
    "\n",
    "print(np.array(model.wv.most_similar(positive=word1)))\n",
    "print(\"**********************************************\")\n",
    "print(np.array(model.wv.most_similar(positive=word2)))\n",
    "print(\"**********************************************\")\n",
    "print(\"Similarity between {} and {} is : {}\".format(word1,word2,model.wv.similarity(word1,word2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding is done.\n"
     ]
    }
   ],
   "source": [
    "def paddedFeatures(features, target_dimension, position):\n",
    "    import numpy as np\n",
    "    padded_features = []\n",
    "    \n",
    "    if position == \"right\":\n",
    "        for sample in features:\n",
    "            zeros = np.zeros((target_dimension-sample.shape[0],sample.shape[1]))\n",
    "            padded_sample = np.concatenate((sample,zeros),axis=0)\n",
    "            padded_features.append(padded_sample)\n",
    "             \n",
    "    if position == \"left\":\n",
    "        for sample in features:\n",
    "            zeros = np.zeros((target_dimension-sample.shape[0],sample.shape[1]))\n",
    "            padded_sample = np.concatenate((zeros,sample),axis=0)\n",
    "            padded_features.append(padded_sample)\n",
    "            \n",
    "    print(\"Padding is done.\")\n",
    "            \n",
    "    return padded_features\n",
    "\n",
    "x_features = paddedFeatures(x_features, target_dimension= max_tokens_lengths, position = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings for a sentence after zero-padding: (50, 40)\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embeddings for a sentence after zero-padding:\",np.array(x_features[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Data Split Train and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y, test_size= split_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "x_train = torch.tensor(x_train).float()\n",
    "y_train = torch.tensor(y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23875, 50, 40]) torch.Size([23875, 2])\n",
      "(Number of sentence),(Number of tokens in each sample), (embedding size for each token)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.size(),y_train.size())\n",
    "print(\"(Number of sentence),(Number of tokens in each sample), (embedding size for each token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(40, 5, batch_first=True)\n",
      "  (linear): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n",
      "Epoch: 0\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.25035995835471525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PROGRAM FILES\\Anaconda\\envs\\staj_projesi\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 1\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.25024279656834625\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 2\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.2502151239118027\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 3\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.25018472687236926\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 4\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.250151634618874\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 5\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.2501539497837346\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 6\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.25016672002270585\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 7\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.25016275821056666\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 8\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.25016915720919664\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 9\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.2501553779684436\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 10\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n",
      "Trained sample:  15000\n",
      "Trained sample:  16000\n",
      "Trained sample:  17000\n",
      "Trained sample:  18000\n",
      "Trained sample:  19000\n",
      "Trained sample:  20000\n",
      "Trained sample:  21000\n",
      "Trained sample:  22000\n",
      "Trained sample:  23000\n",
      "Average loss:  0.25015540701431754\n",
      "\n",
      "Model kaydedildi.\n",
      "\n",
      "Epoch: 11\n",
      "Trained sample:  0\n",
      "Trained sample:  1000\n",
      "Trained sample:  2000\n",
      "Trained sample:  3000\n",
      "Trained sample:  4000\n",
      "Trained sample:  5000\n",
      "Trained sample:  6000\n",
      "Trained sample:  7000\n",
      "Trained sample:  8000\n",
      "Trained sample:  9000\n",
      "Trained sample:  10000\n",
      "Trained sample:  11000\n",
      "Trained sample:  12000\n",
      "Trained sample:  13000\n",
      "Trained sample:  14000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-14c337aa5887>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0msingle_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0msingle_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\PROGRAM FILES\\Anaconda\\envs\\staj_projesi\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\PROGRAM FILES\\Anaconda\\envs\\staj_projesi\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from time import time\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=40, hidden_layer_size=5, output_size=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch = 1\n",
    "        self.num_layers = 1\n",
    "        self.hidden_layer_size = 5\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first = True) #add dropout = 0.2\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        \n",
    "        self.linear2 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(self.num_layers,self.batch,self.hidden_layer_size),\n",
    "                            torch.zeros(self.num_layers,self.batch,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \n",
    "        #print(\"Input Shape: \", input_seq.shape)\n",
    "    \n",
    "        input_seq = input_seq.view(1,max_tokens_lengths,output_size)\n",
    "        #print(\"Expanded LSTM Input Shape: \", input_seq.shape)\n",
    "        \n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n",
    "        #print(\"LSTM output shape: \",lstm_out.shape)\n",
    "        \n",
    "        #print(\"Linear Layer Input Shape: \",lstm_out.view(-1,self.hidden_layer_size).shape)\n",
    "        predictions = self.linear(lstm_out.view(-1,self.hidden_layer_size))\n",
    "        \n",
    "        predictions = self.linear2(predictions)\n",
    "        \n",
    "        #print(\"Predictions shape: \",predictions.shape)\n",
    "        #print(\"Returned values for loss\", predictions[-1].shape)\n",
    "        y = torch.nn.functional.softmax(predictions[-1], dim=0)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "\n",
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "print(model)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "average_loss = []\n",
    "t = time()\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch:\", i)\n",
    "    loss_list = []\n",
    "    \n",
    "    for k in range(y_train.shape[0]):\n",
    "        if k%1000==0:\n",
    "            print(\"Trained sample: \",k)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "       \n",
    "        model.hidden_cell = (torch.zeros(model.num_layers, model.batch, model.hidden_layer_size),\n",
    "                        torch.zeros(model.num_layers, model.batch, model.hidden_layer_size))\n",
    "\n",
    "        y_pred = model(x_train[k])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        single_loss = loss_function(y_pred, y_train[k])\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(single_loss.item())\n",
    "\n",
    "        \n",
    "        \"\"\"if k%100==0:\n",
    "            print(\"Data: \",k)\n",
    "            print(\"Toplam geçen süre:\",round((time() - t) / 60, 2),\" dakika\")\n",
    "            print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "            print(y_pred)\n",
    "            print(y_train[k])\n",
    "            print(\"*************************************************\")\"\"\"\n",
    "        \n",
    "    average_loss.append(sum(loss_list)/len(loss_list))\n",
    "    checkpoint = {'model': LSTM(),'state_dict': model.state_dict(),'optimizer' : optimizer.state_dict()}\n",
    "    print(\"Average loss: \",sum(loss_list)/y_train.shape[0])\n",
    "    torch.save(checkpoint, 'Epoch {} Checkpoint.pth'.format(i))\n",
    "    print(\"\\nModel kaydedildi.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
